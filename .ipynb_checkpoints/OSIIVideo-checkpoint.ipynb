{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2 as cv\n",
    "\n",
    "cv.moveWindow('Frame', 0, 0)\n",
    "cv.moveWindow('FG MaskMOG', 0, 50)\n",
    "cv.moveWindow('FG MaskKNN', 0, 100)\n",
    "\n",
    "\n",
    "backSubMOG = cv.createBackgroundSubtractorMOG2()\n",
    "backSubKNN = cv.createBackgroundSubtractorKNN()\n",
    "\n",
    "capture = cv.VideoCapture('E:\\people_walking.mp4')\n",
    "if not capture.isOpened:\n",
    "    print('Unable to open: ')\n",
    "    exit(0)\n",
    "    \n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    fgMaskMOG = backSubMOG.apply(frame)\n",
    "    fgMaskKNN = backSubKNN.apply(frame)\n",
    "    \n",
    "    cv.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv.putText(frame, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    cv.resizeWindow('Frame', 600,500)\n",
    "    cv.resizeWindow('FG MaskMOG', 600,500)\n",
    "    cv.resizeWindow('FG MaskKNN', 600,500)\n",
    "    \n",
    "    cv.imshow('Frame', frame)\n",
    "    cv.imshow('FG MaskMOG', fgMaskMOG)\n",
    "    cv.imshow('FG MaskKNN', fgMaskKNN)\n",
    "    \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "def showInMovedWindow(winname, img, x, y):\n",
    "    cv2.namedWindow(winname)        # Create a named window\n",
    "    cv2.moveWindow(winname, x, y)   # Move it to (x,y)\n",
    "    cv2.imshow(winname,img, )\n",
    "\n",
    "# showInMovedWindow('named_window',img, 0, 200)\n",
    "\n",
    "cap = cv2.VideoCapture('E:\\people_walking.mp4')\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imshow('frame',gray)\n",
    "    showInMovedWindow('named_window',gray, 0, 1)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cv.moveWindow('frame', 0, 0)\n",
    "cap = cv.VideoCapture('E:\\people_walking.mp4')\n",
    "backSubMOG = cv.createBackgroundSubtractorMOG2()\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "# Create some random colors\n",
    "color = np.random.randint(0,255,(100,3))\n",
    "# Take first frame and find corners in it\n",
    "ret, old_frame = cap.read()\n",
    "if not cap.isOpened:\n",
    "    print('Unable to open: ')\n",
    "    exit(0)\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "p0 = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(old_frame)\n",
    "while(1):\n",
    "    ret,frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgMaskMOG = backSubMOG.apply(frame)\n",
    "    frame_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    # calculate optical flow\n",
    "    p1, st, err = cv.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "    \n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "        a,b = new.ravel()\n",
    "        c,d = old.ravel()\n",
    "        mask = cv.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv.circle(frame,(a,b),5,color[i].tolist(),-1)\n",
    "    img = cv.add(frame,mask)\n",
    "    cv.rectangle(img, (10, 2), (100,20), (255,255,255), -1)\n",
    "    cv.putText(img, str(capture.get(cv.CAP_PROP_POS_FRAMES)), (15, 15),\n",
    "               cv.FONT_HERSHEY_SIMPLEX, 0.5 , (0,0,0))\n",
    "    cv.resizeWindow('frame', 1000,1000)\n",
    "    cv.imshow('frame',img)\n",
    "    cv.imshow('wt', fgMaskMOG)\n",
    "    \n",
    "#     k = cv.waitKey(30) & 0xff\n",
    "#     if k == 27:\n",
    "#         break\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    # Now update the previous frame and previous points\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = good_new.reshape(-1,1,2)\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-2b5g8ysb\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1133: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"opencv_face_detector_uint8.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-1f9b2c0c517c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mgenderList\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Male'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Female'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mfaceNet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaceModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfaceProto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mageNet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mageModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mageProto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mgenderNet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenderModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgenderProto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-2b5g8ysb\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1133: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"opencv_face_detector_uint8.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "#A Gender and Age Detection program by Mahesh Sawant\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "# import argparse\n",
    "\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn=frame.copy()\n",
    "    frameHeight=frameOpencvDnn.shape[0]\n",
    "    frameWidth=frameOpencvDnn.shape[1]\n",
    "    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections=net.forward()\n",
    "    faceBoxes=[]\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence=detections[0,0,i,2]\n",
    "        if confidence>conf_threshold:\n",
    "            x1=int(detections[0,0,i,3]*frameWidth)\n",
    "            y1=int(detections[0,0,i,4]*frameHeight)\n",
    "            x2=int(detections[0,0,i,5]*frameWidth)\n",
    "            y2=int(detections[0,0,i,6]*frameHeight)\n",
    "            faceBoxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn,faceBoxes\n",
    "\n",
    "\n",
    "# parser=argparse.ArgumentParser()\n",
    "# parser.add_argument('--image')\n",
    "\n",
    "# args=parser.parse_args()\n",
    "\n",
    "faceProto=\"opencv_face_detector.pbtxt\"\n",
    "faceModel=\"opencv_face_detector_uint8.pb\"\n",
    "ageProto=\"age_deploy.prototxt\"\n",
    "ageModel=\"age_net.caffemodel\"\n",
    "genderProto=\"gender_deploy.prototxt\"\n",
    "genderModel=\"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList=['Male','Female']\n",
    "\n",
    "faceNet=cv2.dnn.readNet(faceModel,faceProto)\n",
    "ageNet=cv2.dnn.readNet(ageModel,ageProto)\n",
    "genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
    "\n",
    "video=cv2.VideoCapture(args.image if args.image else 0)\n",
    "padding=20\n",
    "while cv2.waitKey(1)<0 :\n",
    "    hasFrame,frame=video.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    \n",
    "    resultImg,faceBoxes=highlightFace(faceNet,frame)\n",
    "    if not faceBoxes:\n",
    "        print(\"No face detected\")\n",
    "\n",
    "    for faceBox in faceBoxes:\n",
    "        face=frame[max(0,faceBox[1]-padding):\n",
    "                   min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)\n",
    "                   :min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds=genderNet.forward()\n",
    "        gender=genderList[genderPreds[0].argmax()]\n",
    "        print(f'Gender: {gender}')\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds=ageNet.forward()\n",
    "        age=ageList[agePreds[0].argmax()]\n",
    "        print(f'Age: {age[1:-1]} years')\n",
    "\n",
    "        cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Detecting age and gender\", resultImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "# import imutils \n",
    "\n",
    "# Initializing the HOG person \n",
    "# detector \n",
    "hog = cv2.HOGDescriptor() \n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) \n",
    "\n",
    "cap = cv2.VideoCapture('E:\\coast.mp4') \n",
    "# cap = cv2.VideoCapture('E:\\istanbul.mp4') \n",
    "\n",
    "while cap.isOpened():\n",
    "    # Reading the video stream \n",
    "    ret, image = cap.read() \n",
    "    if ret: \n",
    "        image = cv2.resize(image, (700, 400))\n",
    "\n",
    "        (regions, _) = hog.detectMultiScale(image, winStride=(4, 4), padding=(0, 0), scale=1.05) \n",
    "\n",
    "        for (x, y, w, h) in regions: \n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2) \n",
    "\n",
    "        cv2.imshow(\"Image\", image) \n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "            break\n",
    "    else: \n",
    "        break\n",
    "\n",
    "cap.release() \n",
    "cv2.destroyAllWindows() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog.detectMultiScale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-2b5g8ysb\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-24defd11abab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mminSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                 \u001b[1;31m#flags = cv2.CV_HAAR_SCALE_IMAGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \t)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.4.0) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-2b5g8ysb\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while(True):\n",
    "\t# Capture frame-by-frame\n",
    "\tret, frame = cap.read()\n",
    "\n",
    "\t# Our operations on the frame come here\n",
    "\tgray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\t# Detect faces in the image\n",
    "\tfaces = faceCascade.detectMultiScale(\n",
    "\t\tgray,\n",
    "\t\tscaleFactor=1.1,\n",
    "\t\tminNeighbors=5,\n",
    "\t\tminSize=(30, 30)\n",
    "\t\t#flags = cv2.CV_HAAR_SCALE_IMAGE\n",
    "\t)\n",
    "\n",
    "\tprint(\"Found {0} faces!\".format(len(faces)))\n",
    "\n",
    "\t# Draw a rectangle around the faces\n",
    "\tfor (x, y, w, h) in faces:\n",
    "\t\tcv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\t# Display the resulting frame\n",
    "\tcv2.imshow('frame', frame)\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "# Read image from your local file system\n",
    "# original_image = cv.imread('D:\\Lena.jpg')\n",
    "original_image = cv.imread('D:\\people.jpg')\n",
    "\n",
    "# Convert color image to grayscale for Viola-Jones\n",
    "grayscale_image = cv.cvtColor(original_image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load the classifier and create a cascade object for face detection\n",
    "face_cascade = cv.CascadeClassifier('D:/haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
    "for (column, row, width, height) in detected_faces:\n",
    "    cv.rectangle(\n",
    "        original_image,\n",
    "        (column, row),\n",
    "        (column + width, row + height),\n",
    "        (0, 255, 0),\n",
    "        2\n",
    "    )\n",
    "cv.imshow('Image', original_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
